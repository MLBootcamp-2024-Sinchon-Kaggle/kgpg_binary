{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TargetEncoder\n",
    "#!pip install -q scikit-learn==1.4\n",
    "# KerasClassifier\n",
    "#!pip install -q --no-deps scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#현재 커널에 설치가 안되어서 삽질..\n",
    "#import sys\n",
    "#import subprocess\n",
    "# 현재 Python 실행 파일 경로를 사용하여 pip 설치 명령 실행\n",
    "#subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"scikeras\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 12:46:49.341570: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, TargetEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and split the data\n",
    "train_origin = pd.read_csv('/Users/jaesolshin/내 드라이브/2024-2/Google ML Bootcamp2024/data/playground1/train.csv')\n",
    "train = train_origin.sample(frac=0.01, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주형 변수를 팩터로 변환 (카테고리형)\n",
    "train.iloc[:,[1,3,4,5,6,7,9]] = train.iloc[:,[1,3,4,5,6,7,9]].astype('category')\n",
    "\n",
    "# 최소-최대 정규화 (Min-Max 스케일링)\n",
    "scaler = MinMaxScaler()\n",
    "train.iloc[:,[2,8,10]] = scaler.fit_transform(train.iloc[:,[2,8,10]])\n",
    "\n",
    "# 이분변수 생성: \"Annual_Premium\" == 2630.0 인 경우\n",
    "train['Annual_Premium_Binary'] = (train['Annual_Premium'] == 2630.0).astype('category')\n",
    "\n",
    "# 로그 변환된 \"Annual_Premium\" 변수 생성\n",
    "train['Annual_Premium_Log'] = np.where(train['Annual_Premium'] > 0, np.log1p(train['Annual_Premium']), 0)\n",
    "\n",
    "# 예측에 필요 없는 'id'와 'Annual_Premium' 변수를 드롭\n",
    "train = train.drop(columns=['id', 'Annual_Premium'])\n",
    "\n",
    "# 원-핫 인코딩 (One-Hot Encoding)\n",
    "train = pd.get_dummies(train, columns=['Gender', 'Driving_License', 'Region_Code', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage', 'Policy_Sales_Channel', 'Annual_Premium_Binary'])\n",
    "\n",
    "# XGBoost에서 발생하는 문제 해결\n",
    "train.columns = train.columns.str.replace('[', '').str.replace(']', '').str.replace('<', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - loss: 0.2831 - val_loss: 0.2642 - learning_rate: 0.0030\n",
      "Epoch 2/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.2702 - val_loss: 0.2645 - learning_rate: 0.0030\n",
      "Epoch 3/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.2697 - val_loss: 0.2629 - learning_rate: 0.0030\n",
      "Epoch 4/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.2675 - val_loss: 0.2639 - learning_rate: 0.0030\n",
      "Epoch 5/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.2665 - val_loss: 0.2637 - learning_rate: 0.0030\n",
      "Epoch 6/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 0.2690 - val_loss: 0.2638 - learning_rate: 0.0030\n",
      "Epoch 7/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.2598 - val_loss: 0.2613 - learning_rate: 3.0000e-04\n",
      "Epoch 8/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.2647 - val_loss: 0.2617 - learning_rate: 3.0000e-04\n",
      "Epoch 9/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2602 - val_loss: 0.2613 - learning_rate: 3.0000e-04\n",
      "Epoch 10/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.2628 - val_loss: 0.2615 - learning_rate: 3.0000e-04\n",
      "Epoch 11/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.2607 - val_loss: 0.2614 - learning_rate: 3.0000e-05\n",
      "Epoch 12/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2611 - val_loss: 0.2614 - learning_rate: 3.0000e-05\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model: 1it [01:17, 77.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.2829 - val_loss: 0.2703 - learning_rate: 0.0030\n",
      "Epoch 2/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2670 - val_loss: 0.2723 - learning_rate: 0.0030\n",
      "Epoch 3/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.2677 - val_loss: 0.2692 - learning_rate: 0.0030\n",
      "Epoch 4/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2690 - val_loss: 0.2704 - learning_rate: 0.0030\n",
      "Epoch 5/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2671 - val_loss: 0.2738 - learning_rate: 0.0030\n",
      "Epoch 6/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.2639 - val_loss: 0.2700 - learning_rate: 0.0030\n",
      "Epoch 7/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.2633 - val_loss: 0.2687 - learning_rate: 3.0000e-04\n",
      "Epoch 8/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.2620 - val_loss: 0.2690 - learning_rate: 3.0000e-04\n",
      "Epoch 9/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2632 - val_loss: 0.2693 - learning_rate: 3.0000e-04\n",
      "Epoch 10/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2637 - val_loss: 0.2692 - learning_rate: 3.0000e-04\n",
      "Epoch 11/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.2617 - val_loss: 0.2693 - learning_rate: 3.0000e-05\n",
      "Epoch 12/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2605 - val_loss: 0.2692 - learning_rate: 3.0000e-05\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model: 2it [02:14, 65.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.2844 - val_loss: 0.2666 - learning_rate: 0.0030\n",
      "Epoch 2/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2724 - val_loss: 0.2662 - learning_rate: 0.0030\n",
      "Epoch 3/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.2729 - val_loss: 0.2641 - learning_rate: 0.0030\n",
      "Epoch 4/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.2707 - val_loss: 0.2637 - learning_rate: 0.0030\n",
      "Epoch 5/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2704 - val_loss: 0.2619 - learning_rate: 0.0030\n",
      "Epoch 6/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.2697 - val_loss: 0.2627 - learning_rate: 0.0030\n",
      "Epoch 7/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.2677 - val_loss: 0.2611 - learning_rate: 0.0030\n",
      "Epoch 8/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2660 - val_loss: 0.2605 - learning_rate: 0.0030\n",
      "Epoch 9/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2673 - val_loss: 0.2649 - learning_rate: 0.0030\n",
      "Epoch 10/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2674 - val_loss: 0.2605 - learning_rate: 0.0030\n",
      "Epoch 11/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2667 - val_loss: 0.2629 - learning_rate: 0.0030\n",
      "Epoch 12/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2638 - val_loss: 0.2604 - learning_rate: 3.0000e-04\n",
      "Epoch 13/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.2623 - val_loss: 0.2605 - learning_rate: 3.0000e-04\n",
      "Epoch 14/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2607 - val_loss: 0.2607 - learning_rate: 3.0000e-04\n",
      "Epoch 15/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2628 - val_loss: 0.2607 - learning_rate: 3.0000e-04\n",
      "Epoch 16/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2602 - val_loss: 0.2604 - learning_rate: 3.0000e-05\n",
      "Epoch 17/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.2618 - val_loss: 0.2603 - learning_rate: 3.0000e-05\n",
      "Epoch 18/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.2634 - val_loss: 0.2603 - learning_rate: 3.0000e-05\n",
      "Epoch 19/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2642 - val_loss: 0.2602 - learning_rate: 3.0000e-05\n",
      "Epoch 20/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2618 - val_loss: 0.2602 - learning_rate: 3.0000e-05\n",
      "Epoch 21/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.2599 - val_loss: 0.2602 - learning_rate: 3.0000e-05\n",
      "Epoch 22/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2629 - val_loss: 0.2602 - learning_rate: 3.0000e-05\n",
      "Epoch 23/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2603 - val_loss: 0.2602 - learning_rate: 3.0000e-06\n",
      "Epoch 24/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2614 - val_loss: 0.2602 - learning_rate: 3.0000e-06\n",
      "Epoch 25/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2612 - val_loss: 0.2602 - learning_rate: 3.0000e-06\n",
      "Epoch 26/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2618 - val_loss: 0.2602 - learning_rate: 3.0000e-07\n",
      "Epoch 27/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2616 - val_loss: 0.2602 - learning_rate: 3.0000e-07\n",
      "Epoch 28/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2648 - val_loss: 0.2602 - learning_rate: 3.0000e-07\n",
      "Epoch 29/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2613 - val_loss: 0.2602 - learning_rate: 3.0000e-08\n",
      "Epoch 30/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2599 - val_loss: 0.2602 - learning_rate: 3.0000e-08\n",
      "Epoch 31/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2627 - val_loss: 0.2602 - learning_rate: 3.0000e-08\n",
      "Epoch 32/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2620 - val_loss: 0.2602 - learning_rate: 3.0000e-09\n",
      "Epoch 33/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2626 - val_loss: 0.2602 - learning_rate: 3.0000e-09\n",
      "Epoch 34/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2622 - val_loss: 0.2602 - learning_rate: 3.0000e-09\n",
      "Epoch 35/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2629 - val_loss: 0.2602 - learning_rate: 3.0000e-10\n",
      "Epoch 36/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2589 - val_loss: 0.2602 - learning_rate: 3.0000e-10\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model: 3it [04:47, 105.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.2838 - val_loss: 0.2652 - learning_rate: 0.0030\n",
      "Epoch 2/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2677 - val_loss: 0.2657 - learning_rate: 0.0030\n",
      "Epoch 3/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.2675 - val_loss: 0.2645 - learning_rate: 0.0030\n",
      "Epoch 4/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.2685 - val_loss: 0.2668 - learning_rate: 0.0030\n",
      "Epoch 5/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.2688 - val_loss: 0.2656 - learning_rate: 0.0030\n",
      "Epoch 6/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.2660 - val_loss: 0.2654 - learning_rate: 0.0030\n",
      "Epoch 7/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.2645 - val_loss: 0.2633 - learning_rate: 3.0000e-04\n",
      "Epoch 8/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.2626 - val_loss: 0.2634 - learning_rate: 3.0000e-04\n",
      "Epoch 9/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.2616 - val_loss: 0.2637 - learning_rate: 3.0000e-04\n",
      "Epoch 10/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.2624 - val_loss: 0.2637 - learning_rate: 3.0000e-04\n",
      "Epoch 11/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.2596 - val_loss: 0.2638 - learning_rate: 3.0000e-05\n",
      "Epoch 12/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.2616 - val_loss: 0.2639 - learning_rate: 3.0000e-05\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model: 4it [05:52, 89.46s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.2817 - val_loss: 0.2697 - learning_rate: 0.0030\n",
      "Epoch 2/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.2744 - val_loss: 0.2684 - learning_rate: 0.0030\n",
      "Epoch 3/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.2680 - val_loss: 0.2690 - learning_rate: 0.0030\n",
      "Epoch 4/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2671 - val_loss: 0.2688 - learning_rate: 0.0030\n",
      "Epoch 5/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2684 - val_loss: 0.2671 - learning_rate: 0.0030\n",
      "Epoch 6/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.2672 - val_loss: 0.2669 - learning_rate: 0.0030\n",
      "Epoch 7/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2653 - val_loss: 0.2698 - learning_rate: 0.0030\n",
      "Epoch 8/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2655 - val_loss: 0.2682 - learning_rate: 0.0030\n",
      "Epoch 9/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.2650 - val_loss: 0.2672 - learning_rate: 0.0030\n",
      "Epoch 10/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2638 - val_loss: 0.2665 - learning_rate: 3.0000e-04\n",
      "Epoch 11/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2622 - val_loss: 0.2663 - learning_rate: 3.0000e-04\n",
      "Epoch 12/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.2634 - val_loss: 0.2662 - learning_rate: 3.0000e-04\n",
      "Epoch 13/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2617 - val_loss: 0.2667 - learning_rate: 3.0000e-04\n",
      "Epoch 14/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2622 - val_loss: 0.2670 - learning_rate: 3.0000e-04\n",
      "Epoch 15/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2615 - val_loss: 0.2667 - learning_rate: 3.0000e-05\n",
      "Epoch 16/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.2603 - val_loss: 0.2667 - learning_rate: 3.0000e-05\n",
      "Epoch 17/40\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.2592 - val_loss: 0.2668 - learning_rate: 3.0000e-05\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model: 5it [07:03, 84.64s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Model</th>\n",
       "      <th>Fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.876749</td>\n",
       "      <td>0.855069</td>\n",
       "      <td>keras-classifier</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.877097</td>\n",
       "      <td>0.854273</td>\n",
       "      <td>keras-classifier</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.855334</td>\n",
       "      <td>keras-classifier</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.877005</td>\n",
       "      <td>0.853496</td>\n",
       "      <td>keras-classifier</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.875962</td>\n",
       "      <td>0.853751</td>\n",
       "      <td>keras-classifier</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy   ROC AUC             Model  Fold\n",
       "0  0.876749  0.855069  keras-classifier     0\n",
       "1  0.877097  0.854273  keras-classifier     1\n",
       "2  0.876923  0.855334  keras-classifier     2\n",
       "3  0.877005  0.853496  keras-classifier     3\n",
       "4  0.875962  0.853751  keras-classifier     4"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# 평가 함수 정의\n",
    "def evaluate_model(y_true, y_pred, y_proba):\n",
    "    return {\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'ROC AUC': roc_auc_score(y_true, y_proba)\n",
    "    }\n",
    "\n",
    "# 모델을 초기화하는 함수 정의\n",
    "# The Keras model with two hidden layers\n",
    "def make_model(meta):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(meta[\"X_shape_\"][1:]))\n",
    "    model.add(keras.layers.Dense(128, kernel_initializer='lecun_normal', activation='selu'))\n",
    "    model.add(keras.layers.Dense(64, kernel_initializer='lecun_normal', activation='selu'))\n",
    "    model.add(keras.layers.Dense(1, kernel_initializer='lecun_normal', activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "\n",
    "# 교차 검증 설정\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "X = train.drop(columns=['Response'])\n",
    "y = train['Response']\n",
    "\n",
    "# 결과를 저장할 리스트 초기화\n",
    "results = []\n",
    "probas = []\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "for fold, (train_index, valid_index) in enumerate(tqdm(skf.split(X, y), desc=\"Training model\")):\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "\n",
    "    # 모델 초기화 (새로운 인스턴스를 생성)\n",
    "    model_instance = KerasClassifier(\n",
    "    get_model,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=keras.optimizers.AdamW(learning_rate=0.003),\n",
    "    validation_split=0.05,\n",
    "    batch_size=64,\n",
    "    validation_batch_size=65536,\n",
    "    epochs=40,\n",
    "    callbacks=[keras.callbacks.ReduceLROnPlateau(patience=3), keras.callbacks.EarlyStopping(patience=5)]\n",
    "    )\n",
    "\n",
    "    # 모델 학습\n",
    "    model_instance.fit(X_train, y_train)\n",
    "\n",
    "    # 예측\n",
    "    valid_y_pred = model_instance.predict(X_valid)\n",
    "    valid_y_proba = model_instance.predict_proba(X_valid)[:, 1]  # 양성 클래스의 확률만 저장\n",
    "\n",
    "    # 평가\n",
    "    metrics = evaluate_model(y_valid, valid_y_pred, valid_y_proba)\n",
    "    metrics.update({\n",
    "        'Model': 'keras-classifier',\n",
    "        'Fold': fold,\n",
    "    })\n",
    "    \n",
    "    results.append(metrics)\n",
    "    probas.append(valid_y_proba)\n",
    "\n",
    "# DataFrame 생성\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: CatBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CatBoost CV: 5it [02:25, 29.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: LightGBM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LightGBM CV: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 11313, number of negative: 80725\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005798 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 798\n",
      "[LightGBM] [Info] Number of data points in the train set: 92038, number of used features: 122\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122917 -> initscore=-1.965096\n",
      "[LightGBM] [Info] Start training from score -1.965096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LightGBM CV: 1it [00:02,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 11313, number of negative: 80725\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005394 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 801\n",
      "[LightGBM] [Info] Number of data points in the train set: 92038, number of used features: 123\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122917 -> initscore=-1.965096\n",
      "[LightGBM] [Info] Start training from score -1.965096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LightGBM CV: 2it [00:03,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 11312, number of negative: 80726\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016669 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 802\n",
      "[LightGBM] [Info] Number of data points in the train set: 92038, number of used features: 123\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122906 -> initscore=-1.965197\n",
      "[LightGBM] [Info] Start training from score -1.965197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LightGBM CV: 3it [00:05,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 11313, number of negative: 80726\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009662 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 802\n",
      "[LightGBM] [Info] Number of data points in the train set: 92039, number of used features: 124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122915 -> initscore=-1.965108\n",
      "[LightGBM] [Info] Start training from score -1.965108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LightGBM CV: 4it [00:07,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 11313, number of negative: 80726\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004710 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 802\n",
      "[LightGBM] [Info] Number of data points in the train set: 92039, number of used features: 124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122915 -> initscore=-1.965108\n",
      "[LightGBM] [Info] Start training from score -1.965108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LightGBM CV: 5it [00:10,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: XGBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XGBoost CV: 5it [00:23,  4.68s/it]\n"
     ]
    }
   ],
   "source": [
    "# 비교검증할 추가모델 리스트\n",
    "models = [\n",
    "    ('CatBoost', CatBoostClassifier(random_state=42, verbose=0)),\n",
    "    ('LightGBM', LGBMClassifier(random_state=42)),\n",
    "    ('XGBoost', XGBClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "for name, model in models:\n",
    "    fold_results = []\n",
    "    print(f\"Training model: {name}\")\n",
    "    for fold, (train_index, valid_index) in enumerate(tqdm(skf.split(X, y), desc=f\"{name} CV\")):\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "\n",
    "        # 모델 학습\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # 예측\n",
    "        valid_y_pred = model.predict(X_valid)\n",
    "        valid_y_proba = model.predict_proba(X_valid)[:, 1]  # 양성 클래스의 확률만 저장\n",
    "\n",
    "        # 평가\n",
    "        metrics = evaluate_model(y_valid, valid_y_pred, valid_y_proba)\n",
    "        metrics.update({'Model': name, 'Fold': fold})\n",
    "        fold_results.append(metrics)\n",
    "\n",
    "    results.extend(fold_results)\n",
    "\n",
    "# DataFrame 생성\n",
    "results_df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Model  Accuracy   ROC AUC\n",
      "0          CatBoost  0.877547  0.866438\n",
      "1          LightGBM  0.877851  0.865248\n",
      "2           XGBoost  0.877555  0.864283\n",
      "3  keras-classifier  0.876747  0.854385\n"
     ]
    }
   ],
   "source": [
    "# 모델별 평균 Accuracy와 평균 ROC AUC 계산\n",
    "summary_df = results_df.groupby('Model')[['Accuracy', 'ROC AUC']].mean().reset_index()\n",
    "\n",
    "# 요약 데이터프레임 출력\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "개선된 버전..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 샘플링\n",
    "train = train_origin.sample(frac=0.01, random_state = 42)\n",
    "\n",
    "# 예측에 필요 없는 'id'와 'Annual_Premium' 변수를 드롭\n",
    "train = train.drop(columns=['id'])\n",
    "\n",
    "# 범주형 변수(2~3개 클래스) 인코딩\n",
    "def encoding(train):\n",
    "    gender_mapping = {'Male': 0, 'Female': 1}\n",
    "    vehicle_age_mapping = {'< 1 Year': 0, '1-2 Year': 1, '> 2 Years': 2}\n",
    "    vehicle_damage_mapping = {'No': 0, 'Yes': 1}\n",
    "\n",
    "    train['Gender'] = train['Gender'].map(gender_mapping)\n",
    "    train['Vehicle_Age'] = train['Vehicle_Age'].map(vehicle_age_mapping)\n",
    "    train['Vehicle_Damage'] = train['Vehicle_Damage'].map(vehicle_damage_mapping)\n",
    "\n",
    "    return train\n",
    "\n",
    "train = encoding(train)\n",
    "\n",
    "'''(같은 건데 왠지 위 방식이 미세하게 잘 나온다)\n",
    "cat_columns_simple = ['Gender','Vehicle_Age','Vehicle_Damage']\n",
    "train[cat_columns_simple] = train[cat_columns_simple].astype('category')\n",
    "train['Gender'] = train['Gender'].cat.codes\n",
    "train['Vehicle_Age'] = train['Vehicle_Age'].cat.codes\n",
    "train['Vehicle_Damage'] = train['Vehicle_Damage'].cat.codes\n",
    "'''\n",
    "\n",
    "#Previously_Insured 교차항\n",
    "'''\n",
    "train['Previously_Insured_Annual_Premium'] = pd.factorize(train['Previously_Insured'].astype(str) + train['Annual_Premium'].astype(str))[0]\n",
    "train['Previously_Insured_Vehicle_Age'] = pd.factorize(train['Previously_Insured'].astype(str) + train['Vehicle_Age'].astype(str))[0]\n",
    "train['Previously_Insured_Vehicle_Damage'] = pd.factorize(train['Previously_Insured'].astype(str) + train['Vehicle_Damage'].astype(str))[0]\n",
    "train['Previously_Insured_Vintage'] = pd.factorize(train['Previously_Insured'].astype(str) + train['Vintage'].astype(str))[0]\n",
    "'''\n",
    "\n",
    "#Age 교차항\n",
    "train['Age_bins'] = pd.cut(train['Age'], bins=7).cat.codes\n",
    "train['Age_x_Vehicle_Age'] = train['Age_bins'] * train['Vehicle_Age']\n",
    "train['Age_x_Vehicle_Damage'] = train['Age_bins'] * train['Vehicle_Damage']\n",
    "train['Age_x_Previously_Insured'] = train['Age_bins'] * train['Previously_Insured']\n",
    "\n",
    "# 범주형 변수 타겟 인코딩\n",
    "cat_columns = ['Region_Code', 'Policy_Sales_Channel', 'Vintage']\n",
    "train.loc[:,cat_columns] = train.loc[:,cat_columns].astype('category')\n",
    "\n",
    "target_encoder = TargetEncoder()\n",
    "train[cat_columns] = target_encoder.fit_transform(train[cat_columns],train['Response'])\n",
    "\n",
    "#수치형 변수 + 타겟 인코딩 변수 표준화\n",
    "scaler = MinMaxScaler()\n",
    "num_columns = ['Age', 'Annual_Premium','Region_Code', 'Policy_Sales_Channel', 'Vintage']\n",
    "train[num_columns] = scaler.fit_transform(train[num_columns])\n",
    "\n",
    "# 예측변수 분리 및 train, valid set 분리\n",
    "X = train.drop(['Response'], axis=1)\n",
    "y = train['Response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의 함수\n",
    "def get_model2(meta):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(meta[\"X_shape_\"][1:]))\n",
    "    model.add(keras.layers.Dense(64, kernel_initializer='he_normal', activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization()),\n",
    "    model.add(keras.layers.Dense(128, kernel_initializer='he_normal', activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization()),\n",
    "    model.add(keras.layers.Dense(256, kernel_initializer='he_normal', activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization()),\n",
    "    model.add(keras.layers.Dense(128, kernel_initializer='he_normal', activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization()),\n",
    "    model.add(keras.layers.Dense(32, kernel_initializer='he_normal', activation='relu'))\n",
    "    model.add(keras.layers.Dense(1, kernel_initializer='he_normal', activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 학습률 스케줄러 함수 정의\n",
    "def lr_scheduler(epoch, lr):\n",
    "    decay_rate = 0.96\n",
    "    decay_step = 10\n",
    "    new_lr = lr * (decay_rate ** (epoch // decay_step))\n",
    "    print(f\"Epoch {epoch}: Learning rate is {new_lr}\")\n",
    "    return new_lr\n",
    "\n",
    "# LearningRateScheduler 콜백 설정\n",
    "lr_schedule = LearningRateScheduler(lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Learning rate is 0.0020000000949949026\n",
      "Epoch 1/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 13ms/step - loss: 0.2815 - val_loss: 0.2655 - learning_rate: 0.0020\n",
      "Epoch 1: Learning rate is 0.0020000000949949026\n",
      "Epoch 2/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 16ms/step - loss: 0.2629 - val_loss: 0.2593 - learning_rate: 0.0020\n",
      "Epoch 2: Learning rate is 0.0020000000949949026\n",
      "Epoch 3/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 16ms/step - loss: 0.2658 - val_loss: 0.2580 - learning_rate: 0.0020\n",
      "Epoch 3: Learning rate is 0.0020000000949949026\n",
      "Epoch 4/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 0.2596 - val_loss: 0.2568 - learning_rate: 0.0020\n",
      "Epoch 4: Learning rate is 0.0020000000949949026\n",
      "Epoch 5/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - loss: 0.2611 - val_loss: 0.2640 - learning_rate: 0.0020\n",
      "Epoch 5: Learning rate is 0.0020000000949949026\n",
      "Epoch 6/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2618 - val_loss: 0.2577 - learning_rate: 0.0020\n",
      "Epoch 6: Learning rate is 0.0020000000949949026\n",
      "Epoch 7/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2626 - val_loss: 0.2574 - learning_rate: 0.0020\n",
      "Epoch 7: Learning rate is 0.0020000000949949026\n",
      "Epoch 8/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2578 - val_loss: 0.2587 - learning_rate: 0.0020\n",
      "Epoch 8: Learning rate is 0.0020000000949949026\n",
      "Epoch 9/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2621 - val_loss: 0.2591 - learning_rate: 0.0020\n",
      "Epoch 9: Learning rate is 0.0020000000949949026\n",
      "Epoch 10/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - loss: 0.2622 - val_loss: 0.2570 - learning_rate: 0.0020\n",
      "Epoch 10: Learning rate is 0.0019200000911951064\n",
      "Epoch 11/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2584 - val_loss: 0.2578 - learning_rate: 0.0019\n",
      "Epoch 11: Learning rate is 0.0018432000651955605\n",
      "Epoch 12/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2606 - val_loss: 0.2585 - learning_rate: 0.0018\n",
      "Epoch 12: Learning rate is 0.001769472062587738\n",
      "Epoch 13/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2586 - val_loss: 0.2566 - learning_rate: 0.0018\n",
      "Epoch 13: Learning rate is 0.00169869314879179\n",
      "Epoch 14/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - loss: 0.2567 - val_loss: 0.2565 - learning_rate: 0.0017\n",
      "Epoch 14: Learning rate is 0.0016307454183697699\n",
      "Epoch 15/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 0.2593 - val_loss: 0.2590 - learning_rate: 0.0016\n",
      "Epoch 15: Learning rate is 0.001565515547990799\n",
      "Epoch 16/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2590 - val_loss: 0.2564 - learning_rate: 0.0016\n",
      "Epoch 16: Learning rate is 0.0015028949081897734\n",
      "Epoch 17/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2562 - val_loss: 0.2578 - learning_rate: 0.0015\n",
      "Epoch 17: Learning rate is 0.001442779116332531\n",
      "Epoch 18/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2614 - val_loss: 0.2567 - learning_rate: 0.0014\n",
      "Epoch 18: Learning rate is 0.0013850679248571395\n",
      "Epoch 19/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - loss: 0.2618 - val_loss: 0.2587 - learning_rate: 0.0014\n",
      "Epoch 19: Learning rate is 0.001329665221273899\n",
      "Epoch 20/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - loss: 0.2582 - val_loss: 0.2574 - learning_rate: 0.0013\n",
      "Epoch 20: Learning rate is 0.0012254194378852844\n",
      "Epoch 21/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - loss: 0.2597 - val_loss: 0.2575 - learning_rate: 0.0012\n",
      "Epoch 21: Learning rate is 0.0011293465733528136\n",
      "Epoch 22/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 0.2586 - val_loss: 0.2574 - learning_rate: 0.0011\n",
      "Epoch 22: Learning rate is 0.0010408057808876038\n",
      "Epoch 23/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2575 - val_loss: 0.2568 - learning_rate: 0.0010\n",
      "Epoch 23: Learning rate is 0.0009592066526412964\n",
      "Epoch 24/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2590 - val_loss: 0.2583 - learning_rate: 9.5921e-04\n",
      "Epoch 24: Learning rate is 0.0008840048611164093\n",
      "Epoch 25/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2564 - val_loss: 0.2572 - learning_rate: 8.8400e-04\n",
      "Epoch 25: Learning rate is 0.0008146988868713379\n",
      "Epoch 26/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2600 - val_loss: 0.2570 - learning_rate: 8.1470e-04\n",
      "Epoch 26: Learning rate is 0.0007508264780044556\n",
      "Epoch 27/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2594 - val_loss: 0.2567 - learning_rate: 7.5083e-04\n",
      "Epoch 27: Learning rate is 0.0006919616997241974\n",
      "Epoch 28/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2592 - val_loss: 0.2566 - learning_rate: 6.9196e-04\n",
      "Epoch 28: Learning rate is 0.000637711876630783\n",
      "Epoch 29/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2556 - val_loss: 0.2565 - learning_rate: 6.3771e-04\n",
      "Epoch 29: Learning rate is 0.0005877152860164642\n",
      "Epoch 30/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2583 - val_loss: 0.2590 - learning_rate: 5.8772e-04\n",
      "Epoch 30: Learning rate is 0.0005199728851318358\n",
      "Epoch 31/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2559 - val_loss: 0.2576 - learning_rate: 5.1997e-04\n",
      "Epoch 31: Learning rate is 0.0004600387229919433\n",
      "Epoch 32/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2550 - val_loss: 0.2567 - learning_rate: 4.6004e-04\n",
      "Epoch 32: Learning rate is 0.0004070128154754638\n",
      "Epoch 33/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2562 - val_loss: 0.2574 - learning_rate: 4.0701e-04\n",
      "Epoch 33: Learning rate is 0.0003600988941192626\n",
      "Epoch 34/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2548 - val_loss: 0.2564 - learning_rate: 3.6010e-04\n",
      "Epoch 34: Learning rate is 0.00031859246063232414\n",
      "Epoch 35/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2555 - val_loss: 0.2565 - learning_rate: 3.1859e-04\n",
      "Epoch 35: Learning rate is 0.0002818702297210693\n",
      "Epoch 36/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2533 - val_loss: 0.2575 - learning_rate: 2.8187e-04\n",
      "Epoch 36: Learning rate is 0.00024938073062896727\n",
      "Epoch 37/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2544 - val_loss: 0.2570 - learning_rate: 2.4938e-04\n",
      "Epoch 37: Learning rate is 0.00022063611888885494\n",
      "Epoch 38/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2561 - val_loss: 0.2570 - learning_rate: 2.2064e-04\n",
      "Epoch 38: Learning rate is 0.00019520472192764278\n",
      "Epoch 39/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2547 - val_loss: 0.2572 - learning_rate: 1.9520e-04\n",
      "Epoch 39: Learning rate is 0.00017270464038848874\n",
      "Epoch 40/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2566 - val_loss: 0.2570 - learning_rate: 1.7270e-04\n",
      "Epoch 40: Learning rate is 0.0001466860981750488\n",
      "Epoch 41/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2539 - val_loss: 0.2573 - learning_rate: 1.4669e-04\n",
      "Epoch 41: Learning rate is 0.000124587333984375\n",
      "Epoch 42/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2553 - val_loss: 0.2574 - learning_rate: 1.2459e-04\n",
      "Epoch 42: Learning rate is 0.00010581781860351561\n",
      "Epoch 43/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2529 - val_loss: 0.2574 - learning_rate: 1.0582e-04\n",
      "Epoch 43: Learning rate is 8.987599868774413e-05\n",
      "Epoch 44/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - loss: 0.2571 - val_loss: 0.2572 - learning_rate: 8.9876e-05\n",
      "Epoch 44: Learning rate is 7.633586906433105e-05\n",
      "Epoch 45/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2530 - val_loss: 0.2572 - learning_rate: 7.6336e-05\n",
      "Epoch 45: Learning rate is 6.483560806274413e-05\n",
      "Epoch 46/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2559 - val_loss: 0.2572 - learning_rate: 6.4836e-05\n",
      "Epoch 46: Learning rate is 5.506789993286132e-05\n",
      "Epoch 47/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2544 - val_loss: 0.2573 - learning_rate: 5.5068e-05\n",
      "Epoch 47: Learning rate is 4.677173114776611e-05\n",
      "Epoch 48/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 0.2581 - val_loss: 0.2572 - learning_rate: 4.6772e-05\n",
      "Epoch 48: Learning rate is 3.972541030883789e-05\n",
      "Epoch 49/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - loss: 0.2541 - val_loss: 0.2573 - learning_rate: 3.9725e-05\n",
      "Epoch 49: Learning rate is 3.374064170837402e-05\n",
      "Epoch 50/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 0.2570 - val_loss: 0.2573 - learning_rate: 3.3741e-05\n",
      "Epoch 50: Learning rate is 2.7511198132324215e-05\n",
      "Epoch 51/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 0.2549 - val_loss: 0.2572 - learning_rate: 2.7511e-05\n",
      "Epoch 51: Learning rate is 2.2431879656982417e-05\n",
      "Epoch 52/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2564 - val_loss: 0.2573 - learning_rate: 2.2432e-05\n",
      "Epoch 52: Learning rate is 1.829034184570312e-05\n",
      "Epoch 53/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2554 - val_loss: 0.2573 - learning_rate: 1.8290e-05\n",
      "Epoch 53: Learning rate is 1.491344571533203e-05\n",
      "Epoch 54/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2556 - val_loss: 0.2574 - learning_rate: 1.4913e-05\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model: 1it [06:51, 411.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Learning rate is 0.0020000000949949026\n",
      "Epoch 1/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - loss: 0.2829 - val_loss: 0.2683 - learning_rate: 0.0020\n",
      "Epoch 1: Learning rate is 0.0020000000949949026\n",
      "Epoch 2/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2650 - val_loss: 0.2676 - learning_rate: 0.0020\n",
      "Epoch 2: Learning rate is 0.0020000000949949026\n",
      "Epoch 3/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2628 - val_loss: 0.2652 - learning_rate: 0.0020\n",
      "Epoch 3: Learning rate is 0.0020000000949949026\n",
      "Epoch 4/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2612 - val_loss: 0.2649 - learning_rate: 0.0020\n",
      "Epoch 4: Learning rate is 0.0020000000949949026\n",
      "Epoch 5/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2614 - val_loss: 0.2642 - learning_rate: 0.0020\n",
      "Epoch 5: Learning rate is 0.0020000000949949026\n",
      "Epoch 6/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.2621 - val_loss: 0.2644 - learning_rate: 0.0020\n",
      "Epoch 6: Learning rate is 0.0020000000949949026\n",
      "Epoch 7/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2617 - val_loss: 0.2634 - learning_rate: 0.0020\n",
      "Epoch 7: Learning rate is 0.0020000000949949026\n",
      "Epoch 8/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2614 - val_loss: 0.2633 - learning_rate: 0.0020\n",
      "Epoch 8: Learning rate is 0.0020000000949949026\n",
      "Epoch 9/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2609 - val_loss: 0.2636 - learning_rate: 0.0020\n",
      "Epoch 9: Learning rate is 0.0020000000949949026\n",
      "Epoch 10/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.2616 - val_loss: 0.2672 - learning_rate: 0.0020\n",
      "Epoch 10: Learning rate is 0.0019200000911951064\n",
      "Epoch 11/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - loss: 0.2620 - val_loss: 0.2640 - learning_rate: 0.0019\n",
      "Epoch 11: Learning rate is 0.0018432000651955605\n",
      "Epoch 12/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - loss: 0.2596 - val_loss: 0.2641 - learning_rate: 0.0018\n",
      "Epoch 12: Learning rate is 0.001769472062587738\n",
      "Epoch 13/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 21ms/step - loss: 0.2616 - val_loss: 0.2659 - learning_rate: 0.0018\n",
      "Epoch 13: Learning rate is 0.00169869314879179\n",
      "Epoch 14/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - loss: 0.2592 - val_loss: 0.2654 - learning_rate: 0.0017\n",
      "Epoch 14: Learning rate is 0.0016307454183697699\n",
      "Epoch 15/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2617 - val_loss: 0.2641 - learning_rate: 0.0016\n",
      "Epoch 15: Learning rate is 0.001565515547990799\n",
      "Epoch 16/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - loss: 0.2594 - val_loss: 0.2650 - learning_rate: 0.0016\n",
      "Epoch 16: Learning rate is 0.0015028949081897734\n",
      "Epoch 17/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - loss: 0.2601 - val_loss: 0.2634 - learning_rate: 0.0015\n",
      "Epoch 17: Learning rate is 0.001442779116332531\n",
      "Epoch 18/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2597 - val_loss: 0.2632 - learning_rate: 0.0014\n",
      "Epoch 18: Learning rate is 0.0013850679248571395\n",
      "Epoch 19/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - loss: 0.2601 - val_loss: 0.2647 - learning_rate: 0.0014\n",
      "Epoch 19: Learning rate is 0.001329665221273899\n",
      "Epoch 20/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2577 - val_loss: 0.2636 - learning_rate: 0.0013\n",
      "Epoch 20: Learning rate is 0.0012254194378852844\n",
      "Epoch 21/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2605 - val_loss: 0.2642 - learning_rate: 0.0012\n",
      "Epoch 21: Learning rate is 0.0011293465733528136\n",
      "Epoch 22/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 0.2577 - val_loss: 0.2639 - learning_rate: 0.0011\n",
      "Epoch 22: Learning rate is 0.0010408057808876038\n",
      "Epoch 23/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - loss: 0.2586 - val_loss: 0.2626 - learning_rate: 0.0010\n",
      "Epoch 23: Learning rate is 0.0009592066526412964\n",
      "Epoch 24/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 0.2564 - val_loss: 0.2659 - learning_rate: 9.5921e-04\n",
      "Epoch 24: Learning rate is 0.0008840048611164093\n",
      "Epoch 25/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - loss: 0.2605 - val_loss: 0.2637 - learning_rate: 8.8400e-04\n",
      "Epoch 25: Learning rate is 0.0008146988868713379\n",
      "Epoch 26/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 0.2592 - val_loss: 0.2633 - learning_rate: 8.1470e-04\n",
      "Epoch 26: Learning rate is 0.0007508264780044556\n",
      "Epoch 27/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - loss: 0.2580 - val_loss: 0.2631 - learning_rate: 7.5083e-04\n",
      "Epoch 27: Learning rate is 0.0006919616997241974\n",
      "Epoch 28/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - loss: 0.2581 - val_loss: 0.2642 - learning_rate: 6.9196e-04\n",
      "Epoch 28: Learning rate is 0.000637711876630783\n",
      "Epoch 29/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 13ms/step - loss: 0.2588 - val_loss: 0.2640 - learning_rate: 6.3771e-04\n",
      "Epoch 29: Learning rate is 0.0005877152860164642\n",
      "Epoch 30/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - loss: 0.2581 - val_loss: 0.2638 - learning_rate: 5.8772e-04\n",
      "Epoch 30: Learning rate is 0.0005199728851318358\n",
      "Epoch 31/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 0.2592 - val_loss: 0.2639 - learning_rate: 5.1997e-04\n",
      "Epoch 31: Learning rate is 0.0004600387229919433\n",
      "Epoch 32/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 21ms/step - loss: 0.2587 - val_loss: 0.2631 - learning_rate: 4.6004e-04\n",
      "Epoch 32: Learning rate is 0.0004070128154754638\n",
      "Epoch 33/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - loss: 0.2537 - val_loss: 0.2628 - learning_rate: 4.0701e-04\n",
      "Epoch 33: Learning rate is 0.0003600988941192626\n",
      "Epoch 34/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - loss: 0.2587 - val_loss: 0.2638 - learning_rate: 3.6010e-04\n",
      "Epoch 34: Learning rate is 0.00031859246063232414\n",
      "Epoch 35/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - loss: 0.2569 - val_loss: 0.2638 - learning_rate: 3.1859e-04\n",
      "Epoch 35: Learning rate is 0.0002818702297210693\n",
      "Epoch 36/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - loss: 0.2584 - val_loss: 0.2635 - learning_rate: 2.8187e-04\n",
      "Epoch 36: Learning rate is 0.00024938073062896727\n",
      "Epoch 37/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - loss: 0.2542 - val_loss: 0.2640 - learning_rate: 2.4938e-04\n",
      "Epoch 37: Learning rate is 0.00022063611888885494\n",
      "Epoch 38/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - loss: 0.2570 - val_loss: 0.2638 - learning_rate: 2.2064e-04\n",
      "Epoch 38: Learning rate is 0.00019520472192764278\n",
      "Epoch 39/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - loss: 0.2565 - val_loss: 0.2638 - learning_rate: 1.9520e-04\n",
      "Epoch 39: Learning rate is 0.00017270464038848874\n",
      "Epoch 40/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - loss: 0.2579 - val_loss: 0.2633 - learning_rate: 1.7270e-04\n",
      "Epoch 40: Learning rate is 0.0001466860981750488\n",
      "Epoch 41/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 0.2563 - val_loss: 0.2636 - learning_rate: 1.4669e-04\n",
      "Epoch 41: Learning rate is 0.000124587333984375\n",
      "Epoch 42/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2563 - val_loss: 0.2634 - learning_rate: 1.2459e-04\n",
      "Epoch 42: Learning rate is 0.00010581781860351561\n",
      "Epoch 43/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - loss: 0.2557 - val_loss: 0.2635 - learning_rate: 1.0582e-04\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model: 2it [13:22, 399.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Learning rate is 0.0020000000949949026\n",
      "Epoch 1/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - loss: 0.2903 - val_loss: 0.2580 - learning_rate: 0.0020\n",
      "Epoch 1: Learning rate is 0.0020000000949949026\n",
      "Epoch 2/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2650 - val_loss: 0.2617 - learning_rate: 0.0020\n",
      "Epoch 2: Learning rate is 0.0020000000949949026\n",
      "Epoch 3/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.2645 - val_loss: 0.2585 - learning_rate: 0.0020\n",
      "Epoch 3: Learning rate is 0.0020000000949949026\n",
      "Epoch 4/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - loss: 0.2623 - val_loss: 0.2573 - learning_rate: 0.0020\n",
      "Epoch 4: Learning rate is 0.0020000000949949026\n",
      "Epoch 5/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - loss: 0.2621 - val_loss: 0.2568 - learning_rate: 0.0020\n",
      "Epoch 5: Learning rate is 0.0020000000949949026\n",
      "Epoch 6/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - loss: 0.2628 - val_loss: 0.2556 - learning_rate: 0.0020\n",
      "Epoch 6: Learning rate is 0.0020000000949949026\n",
      "Epoch 7/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - loss: 0.2628 - val_loss: 0.2585 - learning_rate: 0.0020\n",
      "Epoch 7: Learning rate is 0.0020000000949949026\n",
      "Epoch 8/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2631 - val_loss: 0.2550 - learning_rate: 0.0020\n",
      "Epoch 8: Learning rate is 0.0020000000949949026\n",
      "Epoch 9/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.2633 - val_loss: 0.2556 - learning_rate: 0.0020\n",
      "Epoch 9: Learning rate is 0.0020000000949949026\n",
      "Epoch 10/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.2617 - val_loss: 0.2585 - learning_rate: 0.0020\n",
      "Epoch 10: Learning rate is 0.0019200000911951064\n",
      "Epoch 11/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.2613 - val_loss: 0.2557 - learning_rate: 0.0019\n",
      "Epoch 11: Learning rate is 0.0018432000651955605\n",
      "Epoch 12/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2617 - val_loss: 0.2560 - learning_rate: 0.0018\n",
      "Epoch 12: Learning rate is 0.001769472062587738\n",
      "Epoch 13/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2596 - val_loss: 0.2551 - learning_rate: 0.0018\n",
      "Epoch 13: Learning rate is 0.00169869314879179\n",
      "Epoch 14/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2641 - val_loss: 0.2543 - learning_rate: 0.0017\n",
      "Epoch 14: Learning rate is 0.0016307454183697699\n",
      "Epoch 15/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.2607 - val_loss: 0.2549 - learning_rate: 0.0016\n",
      "Epoch 15: Learning rate is 0.001565515547990799\n",
      "Epoch 16/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.2602 - val_loss: 0.2570 - learning_rate: 0.0016\n",
      "Epoch 16: Learning rate is 0.0015028949081897734\n",
      "Epoch 17/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2607 - val_loss: 0.2548 - learning_rate: 0.0015\n",
      "Epoch 17: Learning rate is 0.001442779116332531\n",
      "Epoch 18/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2587 - val_loss: 0.2546 - learning_rate: 0.0014\n",
      "Epoch 18: Learning rate is 0.0013850679248571395\n",
      "Epoch 19/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2602 - val_loss: 0.2540 - learning_rate: 0.0014\n",
      "Epoch 19: Learning rate is 0.001329665221273899\n",
      "Epoch 20/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2608 - val_loss: 0.2547 - learning_rate: 0.0013\n",
      "Epoch 20: Learning rate is 0.0012254194378852844\n",
      "Epoch 21/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 0.2591 - val_loss: 0.2545 - learning_rate: 0.0012\n",
      "Epoch 21: Learning rate is 0.0011293465733528136\n",
      "Epoch 22/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2614 - val_loss: 0.2556 - learning_rate: 0.0011\n",
      "Epoch 22: Learning rate is 0.0010408057808876038\n",
      "Epoch 23/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2607 - val_loss: 0.2550 - learning_rate: 0.0010\n",
      "Epoch 23: Learning rate is 0.0009592066526412964\n",
      "Epoch 24/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.2588 - val_loss: 0.2544 - learning_rate: 9.5921e-04\n",
      "Epoch 24: Learning rate is 0.0008840048611164093\n",
      "Epoch 25/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.2601 - val_loss: 0.2555 - learning_rate: 8.8400e-04\n",
      "Epoch 25: Learning rate is 0.0008146988868713379\n",
      "Epoch 26/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - loss: 0.2586 - val_loss: 0.2548 - learning_rate: 8.1470e-04\n",
      "Epoch 26: Learning rate is 0.0007508264780044556\n",
      "Epoch 27/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2576 - val_loss: 0.2548 - learning_rate: 7.5083e-04\n",
      "Epoch 27: Learning rate is 0.0006919616997241974\n",
      "Epoch 28/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - loss: 0.2578 - val_loss: 0.2555 - learning_rate: 6.9196e-04\n",
      "Epoch 28: Learning rate is 0.000637711876630783\n",
      "Epoch 29/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - loss: 0.2584 - val_loss: 0.2547 - learning_rate: 6.3771e-04\n",
      "Epoch 29: Learning rate is 0.0005877152860164642\n",
      "Epoch 30/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 13ms/step - loss: 0.2559 - val_loss: 0.2546 - learning_rate: 5.8772e-04\n",
      "Epoch 30: Learning rate is 0.0005199728851318358\n",
      "Epoch 31/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - loss: 0.2623 - val_loss: 0.2548 - learning_rate: 5.1997e-04\n",
      "Epoch 31: Learning rate is 0.0004600387229919433\n",
      "Epoch 32/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - loss: 0.2584 - val_loss: 0.2548 - learning_rate: 4.6004e-04\n",
      "Epoch 32: Learning rate is 0.0004070128154754638\n",
      "Epoch 33/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2576 - val_loss: 0.2555 - learning_rate: 4.0701e-04\n",
      "Epoch 33: Learning rate is 0.0003600988941192626\n",
      "Epoch 34/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2606 - val_loss: 0.2550 - learning_rate: 3.6010e-04\n",
      "Epoch 34: Learning rate is 0.00031859246063232414\n",
      "Epoch 35/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - loss: 0.2553 - val_loss: 0.2553 - learning_rate: 3.1859e-04\n",
      "Epoch 35: Learning rate is 0.0002818702297210693\n",
      "Epoch 36/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2574 - val_loss: 0.2551 - learning_rate: 2.8187e-04\n",
      "Epoch 36: Learning rate is 0.00024938073062896727\n",
      "Epoch 37/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - loss: 0.2570 - val_loss: 0.2549 - learning_rate: 2.4938e-04\n",
      "Epoch 37: Learning rate is 0.00022063611888885494\n",
      "Epoch 38/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 26ms/step - loss: 0.2590 - val_loss: 0.2558 - learning_rate: 2.2064e-04\n",
      "Epoch 38: Learning rate is 0.00019520472192764278\n",
      "Epoch 39/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 21ms/step - loss: 0.2569 - val_loss: 0.2560 - learning_rate: 1.9520e-04\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model: 3it [18:43, 363.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Learning rate is 0.0020000000949949026\n",
      "Epoch 1/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - loss: 0.2847 - val_loss: 0.2612 - learning_rate: 0.0020\n",
      "Epoch 1: Learning rate is 0.0020000000949949026\n",
      "Epoch 2/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.2618 - val_loss: 0.2572 - learning_rate: 0.0020\n",
      "Epoch 2: Learning rate is 0.0020000000949949026\n",
      "Epoch 3/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - loss: 0.2621 - val_loss: 0.2584 - learning_rate: 0.0020\n",
      "Epoch 3: Learning rate is 0.0020000000949949026\n",
      "Epoch 4/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.2636 - val_loss: 0.2617 - learning_rate: 0.0020\n",
      "Epoch 4: Learning rate is 0.0020000000949949026\n",
      "Epoch 5/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.2632 - val_loss: 0.2584 - learning_rate: 0.0020\n",
      "Epoch 5: Learning rate is 0.0020000000949949026\n",
      "Epoch 6/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.2632 - val_loss: 0.2591 - learning_rate: 0.0020\n",
      "Epoch 6: Learning rate is 0.0020000000949949026\n",
      "Epoch 7/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.2627 - val_loss: 0.2572 - learning_rate: 0.0020\n",
      "Epoch 7: Learning rate is 0.0020000000949949026\n",
      "Epoch 8/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.2604 - val_loss: 0.2575 - learning_rate: 0.0020\n",
      "Epoch 8: Learning rate is 0.0020000000949949026\n",
      "Epoch 9/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.2603 - val_loss: 0.2595 - learning_rate: 0.0020\n",
      "Epoch 9: Learning rate is 0.0020000000949949026\n",
      "Epoch 10/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2585 - val_loss: 0.2557 - learning_rate: 0.0020\n",
      "Epoch 10: Learning rate is 0.0019200000911951064\n",
      "Epoch 11/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 0.2586 - val_loss: 0.2571 - learning_rate: 0.0019\n",
      "Epoch 11: Learning rate is 0.0018432000651955605\n",
      "Epoch 12/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - loss: 0.2580 - val_loss: 0.2559 - learning_rate: 0.0018\n",
      "Epoch 12: Learning rate is 0.001769472062587738\n",
      "Epoch 13/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2573 - val_loss: 0.2561 - learning_rate: 0.0018\n",
      "Epoch 13: Learning rate is 0.00169869314879179\n",
      "Epoch 14/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2614 - val_loss: 0.2558 - learning_rate: 0.0017\n",
      "Epoch 14: Learning rate is 0.0016307454183697699\n",
      "Epoch 15/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.2592 - val_loss: 0.2575 - learning_rate: 0.0016\n",
      "Epoch 15: Learning rate is 0.001565515547990799\n",
      "Epoch 16/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2609 - val_loss: 0.2556 - learning_rate: 0.0016\n",
      "Epoch 16: Learning rate is 0.0015028949081897734\n",
      "Epoch 17/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.2554 - val_loss: 0.2565 - learning_rate: 0.0015\n",
      "Epoch 17: Learning rate is 0.001442779116332531\n",
      "Epoch 18/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.2587 - val_loss: 0.2575 - learning_rate: 0.0014\n",
      "Epoch 18: Learning rate is 0.0013850679248571395\n",
      "Epoch 19/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2596 - val_loss: 0.2571 - learning_rate: 0.0014\n",
      "Epoch 19: Learning rate is 0.001329665221273899\n",
      "Epoch 20/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2591 - val_loss: 0.2589 - learning_rate: 0.0013\n",
      "Epoch 20: Learning rate is 0.0012254194378852844\n",
      "Epoch 21/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - loss: 0.2589 - val_loss: 0.2556 - learning_rate: 0.0012\n",
      "Epoch 21: Learning rate is 0.0011293465733528136\n",
      "Epoch 22/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 0.2590 - val_loss: 0.2559 - learning_rate: 0.0011\n",
      "Epoch 22: Learning rate is 0.0010408057808876038\n",
      "Epoch 23/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2573 - val_loss: 0.2562 - learning_rate: 0.0010\n",
      "Epoch 23: Learning rate is 0.0009592066526412964\n",
      "Epoch 24/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.2615 - val_loss: 0.2562 - learning_rate: 9.5921e-04\n",
      "Epoch 24: Learning rate is 0.0008840048611164093\n",
      "Epoch 25/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2553 - val_loss: 0.2570 - learning_rate: 8.8400e-04\n",
      "Epoch 25: Learning rate is 0.0008146988868713379\n",
      "Epoch 26/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2578 - val_loss: 0.2562 - learning_rate: 8.1470e-04\n",
      "Epoch 26: Learning rate is 0.0007508264780044556\n",
      "Epoch 27/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.2578 - val_loss: 0.2556 - learning_rate: 7.5083e-04\n",
      "Epoch 27: Learning rate is 0.0006919616997241974\n",
      "Epoch 28/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.2594 - val_loss: 0.2561 - learning_rate: 6.9196e-04\n",
      "Epoch 28: Learning rate is 0.000637711876630783\n",
      "Epoch 29/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.2572 - val_loss: 0.2559 - learning_rate: 6.3771e-04\n",
      "Epoch 29: Learning rate is 0.0005877152860164642\n",
      "Epoch 30/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.2571 - val_loss: 0.2562 - learning_rate: 5.8772e-04\n",
      "Epoch 30: Learning rate is 0.0005199728851318358\n",
      "Epoch 31/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2565 - val_loss: 0.2560 - learning_rate: 5.1997e-04\n",
      "Epoch 31: Learning rate is 0.0004600387229919433\n",
      "Epoch 32/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2566 - val_loss: 0.2558 - learning_rate: 4.6004e-04\n",
      "Epoch 32: Learning rate is 0.0004070128154754638\n",
      "Epoch 33/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.2585 - val_loss: 0.2561 - learning_rate: 4.0701e-04\n",
      "Epoch 33: Learning rate is 0.0003600988941192626\n",
      "Epoch 34/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.2581 - val_loss: 0.2562 - learning_rate: 3.6010e-04\n",
      "Epoch 34: Learning rate is 0.00031859246063232414\n",
      "Epoch 35/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.2558 - val_loss: 0.2558 - learning_rate: 3.1859e-04\n",
      "Epoch 35: Learning rate is 0.0002818702297210693\n",
      "Epoch 36/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.2573 - val_loss: 0.2558 - learning_rate: 2.8187e-04\n",
      "Epoch 36: Learning rate is 0.00024938073062896727\n",
      "Epoch 37/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.2578 - val_loss: 0.2561 - learning_rate: 2.4938e-04\n",
      "Epoch 37: Learning rate is 0.00022063611888885494\n",
      "Epoch 38/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.2577 - val_loss: 0.2562 - learning_rate: 2.2064e-04\n",
      "Epoch 38: Learning rate is 0.00019520472192764278\n",
      "Epoch 39/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2554 - val_loss: 0.2565 - learning_rate: 1.9520e-04\n",
      "Epoch 39: Learning rate is 0.00017270464038848874\n",
      "Epoch 40/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.2529 - val_loss: 0.2562 - learning_rate: 1.7270e-04\n",
      "Epoch 40: Learning rate is 0.0001466860981750488\n",
      "Epoch 41/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2549 - val_loss: 0.2564 - learning_rate: 1.4669e-04\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model: 4it [23:18, 328.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Learning rate is 0.0020000000949949026\n",
      "Epoch 1/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - loss: 0.2830 - val_loss: 0.2607 - learning_rate: 0.0020\n",
      "Epoch 1: Learning rate is 0.0020000000949949026\n",
      "Epoch 2/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - loss: 0.2653 - val_loss: 0.2656 - learning_rate: 0.0020\n",
      "Epoch 2: Learning rate is 0.0020000000949949026\n",
      "Epoch 3/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.2626 - val_loss: 0.2623 - learning_rate: 0.0020\n",
      "Epoch 3: Learning rate is 0.0020000000949949026\n",
      "Epoch 4/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.2618 - val_loss: 0.2591 - learning_rate: 0.0020\n",
      "Epoch 4: Learning rate is 0.0020000000949949026\n",
      "Epoch 5/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 0.2618 - val_loss: 0.2594 - learning_rate: 0.0020\n",
      "Epoch 5: Learning rate is 0.0020000000949949026\n",
      "Epoch 6/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.2620 - val_loss: 0.2622 - learning_rate: 0.0020\n",
      "Epoch 6: Learning rate is 0.0020000000949949026\n",
      "Epoch 7/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - loss: 0.2614 - val_loss: 0.2595 - learning_rate: 0.0020\n",
      "Epoch 7: Learning rate is 0.0020000000949949026\n",
      "Epoch 8/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.2644 - val_loss: 0.2587 - learning_rate: 0.0020\n",
      "Epoch 8: Learning rate is 0.0020000000949949026\n",
      "Epoch 9/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.2619 - val_loss: 0.2596 - learning_rate: 0.0020\n",
      "Epoch 9: Learning rate is 0.0020000000949949026\n",
      "Epoch 10/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.2596 - val_loss: 0.2611 - learning_rate: 0.0020\n",
      "Epoch 10: Learning rate is 0.0019200000911951064\n",
      "Epoch 11/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.2616 - val_loss: 0.2577 - learning_rate: 0.0019\n",
      "Epoch 11: Learning rate is 0.0018432000651955605\n",
      "Epoch 12/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.2602 - val_loss: 0.2601 - learning_rate: 0.0018\n",
      "Epoch 12: Learning rate is 0.001769472062587738\n",
      "Epoch 13/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.2589 - val_loss: 0.2614 - learning_rate: 0.0018\n",
      "Epoch 13: Learning rate is 0.00169869314879179\n",
      "Epoch 14/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.2615 - val_loss: 0.2588 - learning_rate: 0.0017\n",
      "Epoch 14: Learning rate is 0.0016307454183697699\n",
      "Epoch 15/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.2571 - val_loss: 0.2581 - learning_rate: 0.0016\n",
      "Epoch 15: Learning rate is 0.001565515547990799\n",
      "Epoch 16/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.2595 - val_loss: 0.2587 - learning_rate: 0.0016\n",
      "Epoch 16: Learning rate is 0.0015028949081897734\n",
      "Epoch 17/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.2605 - val_loss: 0.2582 - learning_rate: 0.0015\n",
      "Epoch 17: Learning rate is 0.001442779116332531\n",
      "Epoch 18/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.2609 - val_loss: 0.2578 - learning_rate: 0.0014\n",
      "Epoch 18: Learning rate is 0.0013850679248571395\n",
      "Epoch 19/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.2611 - val_loss: 0.2597 - learning_rate: 0.0014\n",
      "Epoch 19: Learning rate is 0.001329665221273899\n",
      "Epoch 20/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.2583 - val_loss: 0.2579 - learning_rate: 0.0013\n",
      "Epoch 20: Learning rate is 0.0012254194378852844\n",
      "Epoch 21/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.2613 - val_loss: 0.2579 - learning_rate: 0.0012\n",
      "Epoch 21: Learning rate is 0.0011293465733528136\n",
      "Epoch 22/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.2610 - val_loss: 0.2586 - learning_rate: 0.0011\n",
      "Epoch 22: Learning rate is 0.0010408057808876038\n",
      "Epoch 23/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.2598 - val_loss: 0.2580 - learning_rate: 0.0010\n",
      "Epoch 23: Learning rate is 0.0009592066526412964\n",
      "Epoch 24/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.2602 - val_loss: 0.2592 - learning_rate: 9.5921e-04\n",
      "Epoch 24: Learning rate is 0.0008840048611164093\n",
      "Epoch 25/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.2599 - val_loss: 0.2582 - learning_rate: 8.8400e-04\n",
      "Epoch 25: Learning rate is 0.0008146988868713379\n",
      "Epoch 26/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2577 - val_loss: 0.2579 - learning_rate: 8.1470e-04\n",
      "Epoch 26: Learning rate is 0.0007508264780044556\n",
      "Epoch 27/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.2577 - val_loss: 0.2577 - learning_rate: 7.5083e-04\n",
      "Epoch 27: Learning rate is 0.0006919616997241974\n",
      "Epoch 28/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.2574 - val_loss: 0.2582 - learning_rate: 6.9196e-04\n",
      "Epoch 28: Learning rate is 0.000637711876630783\n",
      "Epoch 29/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.2542 - val_loss: 0.2582 - learning_rate: 6.3771e-04\n",
      "Epoch 29: Learning rate is 0.0005877152860164642\n",
      "Epoch 30/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2583 - val_loss: 0.2584 - learning_rate: 5.8772e-04\n",
      "Epoch 30: Learning rate is 0.0005199728851318358\n",
      "Epoch 31/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.2519 - val_loss: 0.2580 - learning_rate: 5.1997e-04\n",
      "Epoch 31: Learning rate is 0.0004600387229919433\n",
      "Epoch 32/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.2572 - val_loss: 0.2583 - learning_rate: 4.6004e-04\n",
      "Epoch 32: Learning rate is 0.0004070128154754638\n",
      "Epoch 33/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.2558 - val_loss: 0.2587 - learning_rate: 4.0701e-04\n",
      "Epoch 33: Learning rate is 0.0003600988941192626\n",
      "Epoch 34/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2534 - val_loss: 0.2585 - learning_rate: 3.6010e-04\n",
      "Epoch 34: Learning rate is 0.00031859246063232414\n",
      "Epoch 35/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - loss: 0.2568 - val_loss: 0.2587 - learning_rate: 3.1859e-04\n",
      "Epoch 35: Learning rate is 0.0002818702297210693\n",
      "Epoch 36/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - loss: 0.2557 - val_loss: 0.2586 - learning_rate: 2.8187e-04\n",
      "Epoch 36: Learning rate is 0.00024938073062896727\n",
      "Epoch 37/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2582 - val_loss: 0.2580 - learning_rate: 2.4938e-04\n",
      "Epoch 37: Learning rate is 0.00022063611888885494\n",
      "Epoch 38/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2569 - val_loss: 0.2586 - learning_rate: 2.2064e-04\n",
      "Epoch 38: Learning rate is 0.00019520472192764278\n",
      "Epoch 39/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - loss: 0.2585 - val_loss: 0.2586 - learning_rate: 1.9520e-04\n",
      "Epoch 39: Learning rate is 0.00017270464038848874\n",
      "Epoch 40/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.2578 - val_loss: 0.2588 - learning_rate: 1.7270e-04\n",
      "Epoch 40: Learning rate is 0.0001466860981750488\n",
      "Epoch 41/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - loss: 0.2585 - val_loss: 0.2587 - learning_rate: 1.4669e-04\n",
      "Epoch 41: Learning rate is 0.000124587333984375\n",
      "Epoch 42/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.2550 - val_loss: 0.2588 - learning_rate: 1.2459e-04\n",
      "Epoch 42: Learning rate is 0.00010581781860351561\n",
      "Epoch 43/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2571 - val_loss: 0.2591 - learning_rate: 1.0582e-04\n",
      "Epoch 43: Learning rate is 8.987599868774413e-05\n",
      "Epoch 44/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.2567 - val_loss: 0.2587 - learning_rate: 8.9876e-05\n",
      "Epoch 44: Learning rate is 7.633586906433105e-05\n",
      "Epoch 45/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2575 - val_loss: 0.2587 - learning_rate: 7.6336e-05\n",
      "Epoch 45: Learning rate is 6.483560806274413e-05\n",
      "Epoch 46/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - loss: 0.2565 - val_loss: 0.2586 - learning_rate: 6.4836e-05\n",
      "Epoch 46: Learning rate is 5.506789993286132e-05\n",
      "Epoch 47/100\n",
      "\u001b[1m684/684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.2552 - val_loss: 0.2587 - learning_rate: 5.5068e-05\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model: 5it [28:31, 342.28s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Model</th>\n",
       "      <th>Fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.876749</td>\n",
       "      <td>0.866675</td>\n",
       "      <td>keras-classifier</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.877662</td>\n",
       "      <td>0.868914</td>\n",
       "      <td>keras-classifier</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.879487</td>\n",
       "      <td>0.870949</td>\n",
       "      <td>keras-classifier</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.877961</td>\n",
       "      <td>0.864625</td>\n",
       "      <td>keras-classifier</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.878439</td>\n",
       "      <td>0.866306</td>\n",
       "      <td>keras-classifier</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy   ROC AUC             Model  Fold\n",
       "0  0.876749  0.866675  keras-classifier     0\n",
       "1  0.877662  0.868914  keras-classifier     1\n",
       "2  0.879487  0.870949  keras-classifier     2\n",
       "3  0.877961  0.864625  keras-classifier     3\n",
       "4  0.878439  0.866306  keras-classifier     4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 교차 검증 설정\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "X = train.drop(columns=['Response'])\n",
    "y = train['Response']\n",
    "\n",
    "# 결과를 저장할 리스트 초기화\n",
    "results = []\n",
    "probas = []\n",
    "\n",
    "# 평가 함수 정의\n",
    "def evaluate_model(y_true, y_pred, y_proba):\n",
    "    return {\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'ROC AUC': roc_auc_score(y_true, y_proba)\n",
    "    }\n",
    "\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "for fold, (train_index, valid_index) in enumerate(tqdm(skf.split(X, y), desc=\"Training model\")):\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "\n",
    "    # 모델 초기화 (새로운 인스턴스를 생성)\n",
    "    model_instance = KerasClassifier(\n",
    "    get_model2,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=keras.optimizers.AdamW(learning_rate=2e-03),\n",
    "    validation_split=0.05,\n",
    "    batch_size=128,\n",
    "    validation_batch_size=65536,\n",
    "    epochs=30,  # Increase the number of epochs for further training\n",
    "    callbacks=[lr_schedule, keras.callbacks.EarlyStopping(patience=20)]\n",
    "    )\n",
    "\n",
    "    # 모델 학습\n",
    "    model_instance.fit(X_train, y_train)\n",
    "\n",
    "    # 예측\n",
    "    valid_y_pred = model_instance.predict(X_valid)\n",
    "    valid_y_proba = model_instance.predict_proba(X_valid)[:, 1]  # 양성 클래스의 확률만 저장\n",
    "\n",
    "    # 평가\n",
    "    metrics = evaluate_model(y_valid, valid_y_pred, valid_y_proba)\n",
    "    metrics.update({\n",
    "        'Model': 'keras-classifier',\n",
    "        'Fold': fold,\n",
    "    })\n",
    "    \n",
    "    results.append(metrics)\n",
    "    probas.append(valid_y_proba)\n",
    "\n",
    "# DataFrame 생성\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8674938487265186"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['ROC AUC'].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
