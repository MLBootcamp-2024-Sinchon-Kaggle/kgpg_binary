{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TargetEncoder\n",
    "#!pip install -q scikit-learn==1.4\n",
    "# KerasClassifier\n",
    "#!pip install -q --no-deps scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#현재 커널에 설치가 안되어서 삽질..\n",
    "#import sys\n",
    "#import subprocess\n",
    "# 현재 Python 실행 파일 경로를 사용하여 pip 설치 명령 실행\n",
    "#subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"scikeras\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 23:37:12.185135: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, TargetEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from scikeras.wrappers import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and split the data\n",
    "train_origin = pd.read_csv('/Users/jaesolshin/내 드라이브/2024-2/Google ML Bootcamp2024/data/playground1/train.csv')\n",
    "train = train_origin.sample(frac=0.01, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_origin.sample(frac=0.05, random_state = 42).set_index('id').astype(str)\n",
    "\n",
    "# 예측변수 분리 및 train, valid set 분리\n",
    "X = train.drop(['Response'], axis=1)\n",
    "y = train['Response']\n",
    "\n",
    "# 훈련 세트와 테스트 세트로 데이터 분할\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, stratify = y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 전체 변수 타겟 인코딩\n",
    "enc = TargetEncoder()\n",
    "X_train = pd.DataFrame(enc.fit_transform(X_train, y_train), \n",
    "                       index=X_train.index, columns=X_train.columns)\n",
    "X_valid = pd.DataFrame(enc.transform(X_valid), index=X_valid.index, \n",
    "                      columns=X_valid.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
      "ROC AUC: 0.8670521083458771\n"
     ]
    }
   ],
   "source": [
    "# The Keras model with two hidden layers\n",
    "def get_model(meta):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(meta[\"X_shape_\"][1:]))\n",
    "    model.add(keras.layers.Dense(64, kernel_initializer='he_normal', activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization()),\n",
    "    model.add(keras.layers.Dense(128, kernel_initializer='he_normal', activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization()),\n",
    "    model.add(keras.layers.Dense(256, kernel_initializer='he_normal', activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization()),\n",
    "    model.add(keras.layers.Dense(128, kernel_initializer='he_normal', activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization()),\n",
    "    model.add(keras.layers.Dense(32, kernel_initializer='he_normal', activation='relu'))\n",
    "    model.add(keras.layers.Dense(1, kernel_initializer='he_normal', activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "relu_model = KerasClassifier(\n",
    "    get_model,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=keras.optimizers.AdamW(learning_rate=0.003),\n",
    "    validation_split=0.05,\n",
    "    batch_size=1024,\n",
    "    validation_batch_size=65536,\n",
    "    epochs=20,\n",
    "    callbacks=[keras.callbacks.ReduceLROnPlateau(patience=3), keras.callbacks.EarlyStopping(patience=5)]\n",
    ")\n",
    "\n",
    "# Execute the pipeline and measure the auc score\n",
    "relu_model.fit(X_train, y_train, verbose=0)\n",
    "valid_preds = relu_model.predict_proba(X_valid)[:, 1]\n",
    "valid_auc = roc_auc_score(y_valid, valid_preds)\n",
    "results.append({'model':'relu', 'AUC':valid_auc})\n",
    "print(\"ROC AUC:\", valid_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Learning rate is 0.0020000000949949026\n",
      "Epoch 1: Learning rate is 0.0020000000949949026\n",
      "Epoch 2: Learning rate is 0.0020000000949949026\n",
      "Epoch 3: Learning rate is 0.0020000000949949026\n",
      "Epoch 4: Learning rate is 0.0020000000949949026\n",
      "Epoch 5: Learning rate is 0.0020000000949949026\n",
      "Epoch 6: Learning rate is 0.0020000000949949026\n",
      "Epoch 7: Learning rate is 0.0020000000949949026\n",
      "Epoch 8: Learning rate is 0.0020000000949949026\n",
      "Epoch 9: Learning rate is 0.0020000000949949026\n",
      "Epoch 10: Learning rate is 0.0019200000911951064\n",
      "Epoch 11: Learning rate is 0.0018432000651955605\n",
      "Epoch 12: Learning rate is 0.001769472062587738\n",
      "Epoch 13: Learning rate is 0.00169869314879179\n",
      "Epoch 14: Learning rate is 0.0016307454183697699\n",
      "Epoch 15: Learning rate is 0.001565515547990799\n",
      "Epoch 16: Learning rate is 0.0015028949081897734\n",
      "Epoch 17: Learning rate is 0.001442779116332531\n",
      "Epoch 18: Learning rate is 0.0013850679248571395\n",
      "Epoch 19: Learning rate is 0.001329665221273899\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step\n",
      "ROC AUC: 0.8665438270577911\n"
     ]
    }
   ],
   "source": [
    "# 모델 정의 함수\n",
    "def get_model(meta):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(meta[\"X_shape_\"][1:]))\n",
    "    model.add(keras.layers.Dense(64, kernel_initializer='he_normal', activation='gelu'))\n",
    "    model.add(keras.layers.BatchNormalization()),\n",
    "    model.add(keras.layers.Dense(128, kernel_initializer='he_normal', activation='gelu'))\n",
    "    model.add(keras.layers.BatchNormalization()),\n",
    "    model.add(keras.layers.Dense(256, kernel_initializer='he_normal', activation='gelu'))\n",
    "    model.add(keras.layers.BatchNormalization()),\n",
    "    model.add(keras.layers.Dense(128, kernel_initializer='he_normal', activation='gelu'))\n",
    "    model.add(keras.layers.BatchNormalization()),\n",
    "    model.add(keras.layers.Dense(32, kernel_initializer='he_normal', activation='gelu'))\n",
    "    model.add(keras.layers.Dense(1, kernel_initializer='he_normal', activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 학습률 스케줄러 함수 정의\n",
    "def lr_scheduler(epoch, lr):\n",
    "    decay_rate = 0.96\n",
    "    decay_step = 10\n",
    "    new_lr = lr * (decay_rate ** (epoch // decay_step))\n",
    "    print(f\"Epoch {epoch}: Learning rate is {new_lr}\")\n",
    "    return new_lr\n",
    "\n",
    "# LearningRateScheduler 콜백 설정\n",
    "lr_schedule = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Recreate the model with the best hyperparameters\n",
    "gelu_model = KerasClassifier(\n",
    "    get_model,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=keras.optimizers.AdamW(learning_rate=2e-03),\n",
    "    validation_split=0.05,\n",
    "    batch_size=128,\n",
    "    validation_batch_size=65536,\n",
    "    epochs=20,  # Increase the number of epochs for further training\n",
    "    callbacks=[lr_schedule, keras.callbacks.EarlyStopping(patience=20)]\n",
    ")\n",
    "\n",
    "# Execute the pipeline and measure the auc score\n",
    "gelu_model.fit(X_train, y_train, verbose=0)\n",
    "\n",
    "valid_preds = gelu_model.predict_proba(X_valid)[:, 1]\n",
    "valid_auc = roc_auc_score(y_valid, valid_preds)\n",
    "results.append({'model':'gelu', 'AUC':valid_auc})\n",
    "print(\"ROC AUC:\", valid_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Learning rate is 0.0020000000949949026\n",
      "Epoch 1: Learning rate is 0.0020000000949949026\n",
      "Epoch 2: Learning rate is 0.0020000000949949026\n",
      "Epoch 3: Learning rate is 0.0020000000949949026\n",
      "Epoch 4: Learning rate is 0.0020000000949949026\n",
      "Epoch 5: Learning rate is 0.0020000000949949026\n",
      "Epoch 6: Learning rate is 0.0020000000949949026\n",
      "Epoch 7: Learning rate is 0.0020000000949949026\n",
      "Epoch 8: Learning rate is 0.0020000000949949026\n",
      "Epoch 9: Learning rate is 0.0020000000949949026\n",
      "Epoch 10: Learning rate is 0.0019200000911951064\n",
      "Epoch 11: Learning rate is 0.0018432000651955605\n",
      "Epoch 12: Learning rate is 0.001769472062587738\n",
      "Epoch 13: Learning rate is 0.00169869314879179\n",
      "Epoch 14: Learning rate is 0.0016307454183697699\n",
      "Epoch 15: Learning rate is 0.001565515547990799\n",
      "Epoch 16: Learning rate is 0.0015028949081897734\n",
      "Epoch 17: Learning rate is 0.001442779116332531\n",
      "Epoch 18: Learning rate is 0.0013850679248571395\n",
      "Epoch 19: Learning rate is 0.001329665221273899\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step\n",
      "ROC AUC: 0.8671588456642852\n"
     ]
    }
   ],
   "source": [
    "# 모델 정의 함수\n",
    "def get_model(meta):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(meta[\"X_shape_\"][1:]))\n",
    "    model.add(keras.layers.Dense(64, kernel_initializer='lecun_normal', activation='selu'))\n",
    "    model.add(keras.layers.BatchNormalization()),\n",
    "    model.add(keras.layers.Dense(128, kernel_initializer='lecun_normal', activation='selu'))\n",
    "    model.add(keras.layers.BatchNormalization()),\n",
    "    model.add(keras.layers.Dense(256, kernel_initializer='lecun_normal', activation='selu'))\n",
    "    model.add(keras.layers.BatchNormalization()),\n",
    "    model.add(keras.layers.Dense(128, kernel_initializer='lecun_normal', activation='selu'))\n",
    "    model.add(keras.layers.BatchNormalization()),\n",
    "    model.add(keras.layers.Dense(32, kernel_initializer='lecun_normal', activation='selu'))\n",
    "    model.add(keras.layers.Dense(1, kernel_initializer='lecun_normal', activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 학습률 스케줄러 함수 정의\n",
    "def lr_scheduler(epoch, lr):\n",
    "    decay_rate = 0.96\n",
    "    decay_step = 10\n",
    "    new_lr = lr * (decay_rate ** (epoch // decay_step))\n",
    "    print(f\"Epoch {epoch}: Learning rate is {new_lr}\")\n",
    "    return new_lr\n",
    "\n",
    "# LearningRateScheduler 콜백 설정\n",
    "lr_schedule = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Recreate the model with the best hyperparameters\n",
    "selu_model = KerasClassifier(\n",
    "    get_model,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=keras.optimizers.AdamW(learning_rate=2e-03),\n",
    "    validation_split=0.05,\n",
    "    batch_size=128,\n",
    "    validation_batch_size=65536,\n",
    "    epochs=20,  # Increase the number of epochs for further training\n",
    "    callbacks=[lr_schedule, keras.callbacks.EarlyStopping(patience=20)]\n",
    ")\n",
    "\n",
    "# Execute the pipeline and measure the auc score\n",
    "selu_model.fit(X_train, y_train, verbose=0)\n",
    "\n",
    "valid_preds = selu_model.predict_proba(X_valid)[:, 1]\n",
    "valid_auc = roc_auc_score(y_valid, valid_preds)\n",
    "results.append({'model':'selu', 'AUC':valid_auc})\n",
    "print(\"ROC AUC:\", valid_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Learning rate is 0.0020000000949949026\n",
      "Epoch 1: Learning rate is 0.0020000000949949026\n",
      "Epoch 2: Learning rate is 0.0020000000949949026\n",
      "Epoch 3: Learning rate is 0.0020000000949949026\n",
      "Epoch 4: Learning rate is 0.0020000000949949026\n",
      "Epoch 5: Learning rate is 0.0020000000949949026\n",
      "Epoch 6: Learning rate is 0.0020000000949949026\n",
      "Epoch 7: Learning rate is 0.0020000000949949026\n",
      "Epoch 8: Learning rate is 0.0020000000949949026\n",
      "Epoch 9: Learning rate is 0.0020000000949949026\n",
      "Epoch 10: Learning rate is 0.0019200000911951064\n",
      "Epoch 11: Learning rate is 0.0018432000651955605\n",
      "Epoch 12: Learning rate is 0.001769472062587738\n",
      "Epoch 13: Learning rate is 0.00169869314879179\n",
      "Epoch 14: Learning rate is 0.0016307454183697699\n",
      "Epoch 15: Learning rate is 0.001565515547990799\n",
      "Epoch 16: Learning rate is 0.0015028949081897734\n",
      "Epoch 17: Learning rate is 0.001442779116332531\n",
      "Epoch 18: Learning rate is 0.0013850679248571395\n",
      "Epoch 19: Learning rate is 0.001329665221273899\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step\n",
      "ROC AUC: 0.8659934080069389\n"
     ]
    }
   ],
   "source": [
    "# 모델 정의 함수\n",
    "def get_model(meta):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(meta[\"X_shape_\"][1:]))\n",
    "    model.add(keras.layers.Dense(64, kernel_initializer='he_normal', activation='mish'))\n",
    "    model.add(keras.layers.BatchNormalization()),\n",
    "    model.add(keras.layers.Dense(128, kernel_initializer='he_normal', activation='mish'))\n",
    "    model.add(keras.layers.BatchNormalization()),\n",
    "    model.add(keras.layers.Dense(256, kernel_initializer='he_normal', activation='mish'))\n",
    "    model.add(keras.layers.BatchNormalization()),\n",
    "    model.add(keras.layers.Dense(128, kernel_initializer='he_normal', activation='mish'))\n",
    "    model.add(keras.layers.BatchNormalization()),\n",
    "    model.add(keras.layers.Dense(32, kernel_initializer='he_normal', activation='mish'))\n",
    "    model.add(keras.layers.Dense(1, kernel_initializer='he_normal', activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "    \n",
    "# 학습률 스케줄러 함수 정의\n",
    "def lr_scheduler(epoch, lr):\n",
    "    decay_rate = 0.96\n",
    "    decay_step = 10\n",
    "    new_lr = lr * (decay_rate ** (epoch // decay_step))\n",
    "    print(f\"Epoch {epoch}: Learning rate is {new_lr}\")\n",
    "    return new_lr\n",
    "\n",
    "# LearningRateScheduler 콜백 설정\n",
    "lr_schedule = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Recreate the model with the best hyperparameters\n",
    "mish_model = KerasClassifier(\n",
    "    get_model,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=keras.optimizers.AdamW(learning_rate=2e-03),\n",
    "    validation_split=0.05,\n",
    "    batch_size=128,\n",
    "    validation_batch_size=65536,\n",
    "    epochs=20,  # Increase the number of epochs for further training\n",
    "    callbacks=[lr_schedule, keras.callbacks.EarlyStopping(patience=20)]\n",
    ")\n",
    "\n",
    "# Execute the pipeline and measure the auc score\n",
    "mish_model.fit(X_train, y_train, verbose=0)\n",
    "\n",
    "valid_preds = mish_model.predict_proba(X_valid)[:, 1]\n",
    "valid_auc = roc_auc_score(y_valid, valid_preds)\n",
    "results.append({'model':'mish', 'AUC':valid_auc})\n",
    "print(\"ROC AUC:\", valid_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model': 'relu', 'AUC': 0.8678594889055563},\n",
       " {'model': 'relu', 'AUC': 0.8670521083458771},\n",
       " {'model': 'gelu', 'AUC': 0.8665438270577911},\n",
       " {'model': 'selu', 'AUC': 0.8671588456642852},\n",
       " {'model': 'mish', 'AUC': 0.8659934080069389}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# 합성음을 재생하여 알림\n",
    "subprocess.run(['say', '-v', 'Karen', 'Process complete'])\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
